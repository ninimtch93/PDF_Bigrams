---
title: "unt2"
output: html_document
date: "2024-11-10"
---

```{r}

install.packages("tidyverse")
install.packages("readtext")
install.packages("quanteda")
install.packages("pdftools")
install.packages("ggplot2")
install.packages("tidytext")

library(tidyverse)
library(readtext)
library(quanteda)
library(pdftools)
library(ggplot2)
library(tidytext)
```

Import the data and compile the articles into a dataframe, one row per sentence.
```{r}

text <- pdf_text("/Users/ninimtchedlishvili/CompText_Jour-main/exercises/assets/pdfs/moley_news.PDF")

writeLines(text, "/Users/ninimtchedlishvili/CompText_Jour-main/exercises/assets/moley_extracted/moley_news.txt")


```

```{r}

file_path <- "/Users/ninimtchedlishvili/CompText_Jour-main/exercises/assets/moley_extracted/moley_news.txt"

text_data <- readLines(file_path)

text_combined <- paste(text_data, collapse = "\n")

documents <- strsplit(text_combined, "End of Document")[[1]]

output_dir <- "/Users/ninimtchedlishvili/CompText_Jour-main/exercises/assets/moley_extracted/"
for (i in seq_along(documents)) {
  output_file <- file.path(output_dir, paste0("moleytext_extracted_", i, ".txt"))
  writeLines(documents[[i]], output_file)
}

cat("Files created:", length(documents), "\n")

```


```{r}

moley_index <- read_lines("/Users/ninimtchedlishvili/CompText_Jour-main/exercises/assets/moley_extracted/moleytext_extracted_1.txt")
extracted_lines <- moley_index[16:219]

cat(extracted_lines, sep = "\n")

extracted_lines <- extracted_lines |> 
  as.data.frame() 

```

Step 1
```{r}

cleaned_data <- extracted_lines |>
  mutate(
      trimmed_line = str_trim(extracted_lines),  

    is_title = str_detect(trimmed_line, "^\\d+\\. "),  

    is_date = str_detect(trimmed_line, "\\b\\w{3} \\d{1,2}, \\d{4}\\b")
  )

aligned_data <- cleaned_data |>
  mutate(
    date = ifelse(lead(is_date, 1), lead(trimmed_line, 1), NA_character_)  # Shift date to title's row
  ) |>
  filter(is_title) |>
  select(trimmed_line, date)

final_data <- aligned_data |>
  rename(
    title = trimmed_line,
    date = date
  )

final_data <- separate(data = final_data, col = date, into = c("date2", "publication"), sep = "  ", extra = "merge", fill = "right")
final_data <- final_data |> 
  mutate(date = as.Date(date2,format = "%b %d, %Y")) |> 
  mutate(title =str_remove(title, "^\\d+\\. ")) |> 
  subset(select = -(date2)) |> 
  mutate(index = row_number()) |> 
  select(index, date, title, publication)

write_csv(final_data, "/Users/ninimtchedlishvili/CompText_Jour-main/exercises/assets/moley_final_data.csv")

```
Compile text into dataframe

```{r include=FALSE}

files <- list.files("/Users/ninimtchedlishvili/CompText_Jour-main/exercises/assets/moley_extracted", pattern="*.txt") %>% 
  as.data.frame() |> 
  rename(filename = 1) |> 
  #create an index with the file name
 mutate(index = str_extract(filename, "\\d+")) |> 
  mutate(index = as.numeric(index))


final_index <- final_data |> 
  inner_join(files, c("index")) |> 

  mutate(filepath = paste0("/Users/ninimtchedlishvili/CompText_Jour-main/exercises/assets/moley_extracted/", filename))
head(final_index)


```
Compiler


```{r}

create_article_text <- function(row_value) {
  
  temp <- final_index %>%
    slice(row_value)

  temp_filename <- temp$filename
  
  articles_df_temp <- read_lines(temp$filepath) %>%
    as_tibble() %>%
    mutate(filename = temp_filename)
 
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}

```

```{r}
articles_df <- tibble()

row_values <- 1:nrow(final_index)

lapply(row_values, create_article_text)

articles_df <- articles_df %>%
  select(filename, sentence=value) %>%
  inner_join(final_index)

write.csv(articles_df, "/Users/ninimtchedlishvili/CompText_Jour-main/exercises/assets/moley_df2.csv")

```

```{r}

bigrams <- articles_df %>%
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2)


```

```{r}
bigrams_separated <- bigrams %>%
  separate(bigram, into = c("word1", "word2"), sep = " ")


```

```{r}

filtered_bigrams <- bigrams_separated %>%
  # Filter out unwanted words and bigrams containing numeric characters
  filter(!word1 %in% c("https", "moley", "www.", "alt", "m.org", "language", "english", "load", "data", "columbia", "university", "publication", "date", "york", "times","en", "wikipedia.", "org", "wiki", "news", "service", "inaugural", "address"),
         !word2 %in% c("https", "moley", "www.", "alt", "m.org", "language", "english", "load", "data", "columbia", "university", "publication", "date", "york", "times","en", "wikipedia.", "org", "wiki", "news", "service", "inaugural", "address"),
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d"),
         !word1 %in% stop_words$word,  # Remove stop words from word1
         !word2 %in% stop_words$word) %>%  # Remove stop words from word2
  unite(bigram, word1, word2, sep = " ")

```

```{r}
bigram_counts <- filtered_bigrams %>%
  count(bigram, sort = TRUE) %>%
  slice_max(n, n = 20)

bigram_counts

```



```{r}
Moley_bigram_plot <- bigram_counts %>% 
  
ggplot(aes(x = bigram, y = n,fill = n)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  labs(title = "Total Two-Word Phrases in the Raymond Moley Articles on the New Deal and U.S. Economic Policy",
       subtitle = " ",
       caption = "Bigram analysis. Graphic by Nini Mtchedlishvili",
       y="Total Number",
       x="Two-Word Phrases")

Moley_bigram_plot


```




```{r}

nrc_sentiments <- get_sentiments("nrc")
afinn_sentiments <- get_sentiments("afinn")

nrc_sentiments %>% count(sentiment)

nrc_sentiments %>% 
  group_by(word) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  distinct()

```

```{r}

sentiments_all <- text_tokenized %>%
  inner_join(nrc_sentiments) 



sentiments_all %>% 
  group_by(word) %>% 
    count(sentiment) %>% 
  arrange(desc(n))

```


```{r}

sentiments_all <- text_tokenized %>%
  inner_join(nrc_sentiments) %>%
  count(sentiment, sort = TRUE) %>% 
  mutate(pct_total =round(n/sum(n), digits=2))

sentiments_all


```

```{r}
moley_plot <- sentiments_all %>% 
  ggplot(aes(x = sentiment, y = n,fill = n)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  labs(title = "Total Sentiment in the Raymond Moley Articles on the New Deal and U.S. Economic Policy",
       subtitle = " ",
       caption = "Sentiment analysis. Graphic by Nini Mtchedlishvili",
       y="Score",
       x="total sentiment score")

moley_plot + scico::scale_fill_scico(palette = "vik")

moley_plot

```


This analysis focuses most frequently used bigrams/two-word phrases and sentiment patterns in a series of articles about topics related to the New Deal. The bigram analysis (first chart/visualization) highlights the most frequently occurring two-word phrases. "Gold standard" and "White House" are the top bigrams, with frequencies of approximately 43 and 39, respectively. Other significant two-word phrases include "economic recovery" and "bank holiday," each appearing about 15 times. These phrases refer to important themes of economic stabilization and recovery, which must have been central to the New Deal’s mission. "Bank holiday," on the other hand, refers to the temporary closure of banks to prevent massive withdrawals, a key early action taken by Roosevelt to restore public confidence in the financial system. This focus on economic recovery measures indicates that these articles were examining both the immediate and long-term impacts of Roosevelt’s interventions.Additionally, phrases like "Keynesian myth" and "federal government" appear frequently, pointing to critical discourse on economic theories and federal policy.

The sentiment analysis (second chart) provides insights into the emotional tone within these articles. Positive sentiment is the highest, with a score nearing 3,800, suggesting that many articles include supportive or optimistic language regarding economic and political topics. Trust and anticipation are also significant, with scores around 2,200 and 1,700, respectively, indicating an underlying sense of reliability and forward-looking attitudes in the narrative. However, there is also a substantial presence of negative sentiment (around 1,500) and sadness (about 1,000), suggesting some critique or pessimism, likely in response to challenges or failures associated with the policies discussed. Lower but notable scores for emotions like "fear" and "disgust" suggest a degree of skepticism or dissatisfaction in certain articles, maybe regarding contentious political and economic issues.

In conclusion, this analysis reveals a complex narrative in the articles, with both high-frequency supportive sentiments and significant critical tones, reflecting the detailed discourse surrounding economic policies and government intervention in the New Deal era.